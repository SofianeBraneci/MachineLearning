{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    \n",
    "    \"\"\"Basic implementation of Q learning algorithm\"\"\"\n",
    "    def __init__(self,lr=0.1, gamma=0.9):\n",
    "        \"\"\"lr: learning rate\n",
    "           gamma: the importance of the incoming values\n",
    "        \"\"\"\n",
    "        self.lr = lr\n",
    "        self.gamma = gamma\n",
    "        self.params = {'lr': self.lr, 'gamma':self.gamma}\n",
    "        self.x = 0\n",
    "        self.y = 0\n",
    "        self.n_actions = 4\n",
    "        self.n_states = 6\n",
    "        self.Q = np.zeros((6,4))\n",
    "        self.env= [[0,1,0],[2,-3,10]]\n",
    "        self.actions = [ [-1,0],[1,0],[0,-1],[0,1] ]\n",
    "        self.states = [[1,2,3],[4,5,6]]\n",
    "        \n",
    "        \n",
    "    def perform_action(self,action):\n",
    "        self.y = max(0, min(self.y + self.actions[action][0],2))\n",
    "        self.x = max(0, min(self.x + self.actions[action][1],2))\n",
    "\n",
    "        return self.states[self.x][self.y], self.env[self.x][self.y]\n",
    "    \n",
    "    def choose_action(self,st,eps):\n",
    "        if np.random.uniform(0, 1) < eps:\n",
    "            action = np.random.randint(0, 2)\n",
    "        else: # Or greedy action\n",
    "            action = np.argmax(self.Q[st-1])\n",
    "        return action\n",
    "    def is_finished(self):\n",
    "        return self.env[self.x][self.y] == 10\n",
    "    \n",
    "    \n",
    "    \n",
    "    def rest(self):\n",
    "        self.env= [[0,1,0],[2,-3,10]]\n",
    "    \n",
    "    \n",
    "    def execute(self):\n",
    "        st = 0\n",
    "        for _ in range(7000):\n",
    "            at = self.choose_action(st,0.4)\n",
    "            st1 ,r = self.perform_action(at)\n",
    "            at1 = self.choose_action(st,0.0)\n",
    "            self.Q[st][at] = self.Q[st][at] + 0.1*(r+ 0.9*(self.Q[st1][at1] + self.Q[st][at]))\n",
    "            st = st1\n",
    "    \n",
    "    def fun(self):\n",
    "        for _ in range(5):\n",
    "            \n",
    "            s, r = self.perform_action(np.random.randint(0,3))\n",
    "            print(self.x,self.y)\n",
    "            self.env[self.x][self.y] = 'X'\n",
    "            print(self.env)\n",
    "            print('reward: %s , state: %s '% (r,s))\n",
    "            self.rest()\n",
    "   \n",
    "        \n",
    "    \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = Agent()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.execute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.00000000e+000, 1.00000000e-001, 0.00000000e+000,\n",
       "        0.00000000e+000],\n",
       "       [1.19453304e+299, 2.77218228e+300, 0.00000000e+000,\n",
       "        0.00000000e+000],\n",
       "       [1.19614305e+299, 2.58251287e+301, 0.00000000e+000,\n",
       "        0.00000000e+000],\n",
       "       [1.99954057e+300, 2.48405085e+302, 0.00000000e+000,\n",
       "        0.00000000e+000],\n",
       "       [0.00000000e+000, 0.00000000e+000, 0.00000000e+000,\n",
       "        0.00000000e+000],\n",
       "       [0.00000000e+000, 0.00000000e+000, 0.00000000e+000,\n",
       "        0.00000000e+000]])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
